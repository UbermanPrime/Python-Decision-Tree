{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4.5 Decision Tree\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this iPython notebook, we will implement a C4.5 Decision Tree algorithm for machine learning from the scratch.  This is my own Python implementation that utilizes Pandas DataFrame as input data.  The popular Python Scikit-learn decision tree requires Numpy as input that is pure numeric.  Scikit-learn users are required to convert categorical input features to numeric, say color red as 0, blue as 1, green as 2, and the library treats all features the same as continuous numbers.  But I think there is no superior of one color over the others.  Therefore, I implement my own Python version of decision tree that does not require any preprocessing for both numeric and categorical input features, and would process both types of features differently in the internal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# A structure to store information of a node in a tree \n",
    "class Tree:\n",
    "    # return prediction if the node is a leaf\n",
    "    prediction = None\n",
    "    # feature for splitting at a node\n",
    "    feature = None\n",
    "    # class or threshold of the feature for splitting\n",
    "    splitPoint = None\n",
    "    # a point to left tree node or leaf \n",
    "    left = None\n",
    "    # a point to right tree node or leaf \n",
    "    right = None\n",
    "    # store entropy for analysis\n",
    "    entropy = None\n",
    "    # size of the node or leaft\n",
    "    nCount  = None\n",
    "    # distribution of target values at the node, e.g. [2 8]\n",
    "    distribution = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Information Gain\n",
    "\n",
    "In short, the entropy $H$ measures the impurity of our target variable $y$, which values are in $k$ classes (boolean counts as 2):\n",
    "\n",
    "$$ H(y)=-\\sum _{ k }P(y=k){ log }_{ 2 }P(y=k) $$\n",
    "Our algorithm builds decision tree with greedy approach, which, at each node, tries to find a split with maximum information gain on our target.  Suppose we have a target $y$ in size $m$ that is going to split into two subsets ${ y }_{ left }$ and ${ y }_{ right }$, with size ${ m }_{ left }$ and ${ m }_{ right }$ respectively, the information gain is defined as:\n",
    "\n",
    "$$IG = H(y)-\\left[ \\frac { { m }_{ left } }{ m } H({ y }_{ left })+\\frac { { m }_{ right } }{ m } H({ y }_{ right }) \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# entropy funciton to support multiple classes\n",
    "# y takes Pandas Series or 1D Numpy array\n",
    "def entropy(y):\n",
    "    dummy, count = np.unique(y.values, return_counts=True)\n",
    "    p = 1.*count/len(y)\n",
    "    h = sum(-p*np.log2(p))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def informationGain(y, y_left, y_right):\n",
    "    H_mother = entropy(y)\n",
    "    H_left = entropy(y_left)\n",
    "    H_right = entropy(y_right)\n",
    "    H_childern = 1.*(len(y_left) * H_left + len(y_right) * H_right) / len(y)\n",
    "    return H_mother - H_childern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find All Possible Split Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test if an input feature is categorical or not based on its data type\n",
    "# input data X can be Pandas DataFrame, or a row in DataFrame\n",
    "def isCategorical(X, feature):\n",
    "    dtype = type(X[feature])\n",
    "    if dtype == pd.core.series.Series:\n",
    "        dtype = X[feature].dtype\n",
    "\n",
    "    return (dtype==str or dtype==object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all possible split points for a feature of both categorical or numeric\n",
    "def allSplitPoints(X, y, feature):\n",
    "    # if catgegorical, just returns all unique classes\n",
    "    if isCategorical(X, feature):\n",
    "        return X[feature].unique()\n",
    "    \n",
    "    # if numeric, sort on input feature, return thresholds where change of classes on y occurs \n",
    "    thresholds = []\n",
    "    view = pd.concat([X[feature], y], axis=1).copy()\n",
    "    view.sort_values(feature, inplace=True)\n",
    "    iterator = view.iterrows()\n",
    "    last_row = iterator.next()[1]\n",
    "    for dummy, row in (view.iterrows()):\n",
    "        if row[y.name] != last_row[y.name]:\n",
    "            thresholds.append((row[feature]+last_row[feature])/2.)\n",
    "        last_row = row\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Best Feature to Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common function for all feature values comparision in the package\n",
    "def compare(X, feature, splitPoint):\n",
    "    # if feature is categorical, test if it falls in the splitPoint as class\n",
    "    if isCategorical(X, feature):\n",
    "        return X[feature] == splitPoint\n",
    "    \n",
    "    # if features is numeric, test if it's within the splitPoint as threshold\n",
    "    return X[feature] <= splitPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the best split point for a feature based on information gain\n",
    "def findBestSplitPoint(X, y, feature):\n",
    "    best_ig = 0\n",
    "    best_splitPoint = None\n",
    "    \n",
    "    # loop for all possible split points of a feature\n",
    "    for splitPoint in allSplitPoints(X, y, feature):\n",
    "        y_left = y.loc[compare(X,feature,splitPoint)]\n",
    "        y_right = y.loc[np.logical_not(compare(X,feature,splitPoint))]\n",
    "        ig = informationGain(y, y_left, y_right)\n",
    "        # test if the current ig is better than previous, beware rounding\n",
    "        if ig - best_ig > 0.0001:\n",
    "            best_ig = ig\n",
    "            best_splitPoint = splitPoint\n",
    "\n",
    "    # return None, if no information gain at all\n",
    "    return (best_splitPoint, best_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the best feature to split at a node\n",
    "# randomFeature is reserved for random forest\n",
    "def findBestFeature(X, y, randomFeatrue=False):\n",
    "    best_feature = None\n",
    "    best_splitPoint = None\n",
    "    best_ig = 0\n",
    "    \n",
    "    # for random forest, randomly pick sqrt(n) features to test\n",
    "    if randomFeatrue:\n",
    "        n = int(len(X.columns)**0.5)\n",
    "        columns = np.random.choice(X.columns, n, replace=False)\n",
    "    else:\n",
    "        # otherwise, test all features\n",
    "        columns = X.columns\n",
    "\n",
    "    for feature in columns:\n",
    "        # for each feature, find it's best split point and ig\n",
    "        splitPoint, ig = findBestSplitPoint(X, y, feature)\n",
    "        # test if the current ig is better than previous, beware rounding\n",
    "        if ig - best_ig > 0.0001:\n",
    "            best_ig = ig\n",
    "            best_feature = feature\n",
    "            best_splitPoint = splitPoint\n",
    "\n",
    "    # return None if no information gain at all\n",
    "    return (best_feature, best_splitPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data and Build Tree Recursively\n",
    "\n",
    "Computer scientists love recursion, it's fun to implement recursion for decision tree.  Our C4.5 decision tree implements the following stopping criteria:\n",
    "\n",
    "* all targets are in the same class\n",
    "* cannot find splits with positive information gain\n",
    "* tree depth grows to maximum limit\n",
    "* leaf size meets minimum size limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataset (X,y) given a feature and split pont to split\n",
    "def splitData(X, y, feature, splitPoint):\n",
    "    # the splitted dataset should be returned as reference, for performance consideration\n",
    "    # beware the Pandas characteristic, always use \".loc[]\"\n",
    "    X_left = X.loc[compare(X,feature,splitPoint)]\n",
    "    y_left = y.loc[compare(X,feature,splitPoint)]\n",
    "    X_right = X.loc[np.logical_not(compare(X,feature,splitPoint))]\n",
    "    y_right = y.loc[np.logical_not(compare(X,feature,splitPoint))]\n",
    "    return (X_left, y_left, X_right, y_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def voteMajority(y):\n",
    "    # beware the weird characteristic of Pandas mode function\n",
    "    # use Scipy instead\n",
    "    return mode(y)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build tree recursively, return the node built\n",
    "# X takes Pandas DataFrame, elements can be int, float or string\n",
    "# y takes Pandas Series, elements can be int, float or string\n",
    "# depth was used internally, can be ignored\n",
    "# maxDepth is a model parameter to control the maximum depth of the tree\n",
    "# minLeafSize is another model parameter, which the leaf size to prevent overfitting\n",
    "# randomFeature is reserved for random forest, when on, random features are selected to split\n",
    "def buildTree(X, y, depth=1, maxDepth=50, minLeafSize=5, randomFeatrue=False):\n",
    "#    print \"depth #%d\" % depth    \n",
    "    tree = Tree()\n",
    "    tree.entropy = entropy(y)\n",
    "    tree.ncount = y.count()\n",
    "    tree.distribution = y.value_counts(sort=False).values\n",
    "\n",
    "    # stop building when node size decreased to minimum leaf size limit\n",
    "    if len(y)<=minLeafSize:\n",
    "        tree.prediction = voteMajority(y)\n",
    "        return tree\n",
    "\n",
    "    # stop building when all target values are the same\n",
    "    if y.max()==y.min():\n",
    "        tree.prediction = voteMajority(y)\n",
    "        return tree\n",
    "\n",
    "    # stop building when the node depth reaches the maxmium tree depth limit\n",
    "    if depth==maxDepth:\n",
    "        tree.prediction = voteMajority(y)\n",
    "        return tree\n",
    "\n",
    "    # try to find the best feature and split point at this node\n",
    "    feature, splitPoint = findBestFeature(X, y, randomFeatrue)\n",
    "    \n",
    "    # stop building when no information gain for any splits\n",
    "    if feature==None:\n",
    "        tree.prediction = voteMajority(y)\n",
    "        return tree\n",
    "\n",
    "    tree.feature = feature\n",
    "    tree.splitPoint = splitPoint\n",
    "    \n",
    "    # split the node dataset (X,y) into two, and call recursively\n",
    "    (X_left, y_left, X_right, y_right) = splitData(X, y, feature, splitPoint)\n",
    "    depth = depth + 1\n",
    "    tree.left=buildTree(X_left, y_left, depth, \n",
    "                        maxDepth=maxDepth, minLeafSize=minLeafSize, \n",
    "                        randomFeatrue=randomFeatrue)\n",
    "    tree.right=buildTree(X_right, y_right, depth, \n",
    "                        maxDepth=maxDepth, minLeafSize=minLeafSize, \n",
    "                        randomFeatrue=randomFeatrue)\n",
    "    \n",
    "    # return the tree node built\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return the prediction for an instance of a built tree\n",
    "# row takes a row from a Pandas DataFrame like X\n",
    "# tree is a built tree returned from the buildTree function\n",
    "def predictTree(row, tree):\n",
    "    if tree.prediction != None:\n",
    "        return tree.prediction\n",
    "\n",
    "    if compare(row, tree.feature, tree.splitPoint):\n",
    "        return predictTree(row, tree.left)\n",
    "    \n",
    "    return predictTree(row, tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score a tree model, returns accuracy in percentage\n",
    "# X is input that takes Pandas DataFrame\n",
    "# y is target that takes Pandas Series\n",
    "def scoreTree(X, y, tree):\n",
    "    p = X.apply(lambda x: predictTree(x, tree), axis=1)\n",
    "    return np.mean(p==y*1.)*100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Tree\n",
    "\n",
    "We use Graphviz to visualize the built decision tree in this package.  Follow [this URL](  https://bobswift.atlassian.net/wiki/display/GVIZ/How+to+install+Graphviz+software) to install the Gaphviz software in your machine.  \n",
    "Run `pip install graphviz` under the Anaconda environment to install the Python Graphviz library.  \n",
    "You should be able to display the trained decision tree on iPython Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from uuid import uuid4\n",
    "\n",
    "def drawGraph(graph, tree):\n",
    "    node_id = uuid4().hex\n",
    "    if tree.prediction != None:        \n",
    "        graph.node(node_id, shape=\"box\", \n",
    "                 label=\"%s\\nentropy = %.4f\\nsampels = %d\\ny %s\" \n",
    "                 % (tree.prediction, tree.entropy, tree.ncount, \n",
    "                    tree.distribution))\n",
    "        return node_id\n",
    "    \n",
    "    graph.node(node_id, shape=\"box\", \n",
    "             label=\"%s | %s\\nentropy = %.4f\\nsamples = %d\\ny %s\" \n",
    "             % (tree.feature, tree.splitPoint, tree.entropy, tree.ncount, \n",
    "                tree.distribution))\n",
    "    left_id = drawGraph(graph, tree.left)\n",
    "    graph.edge(node_id, left_id, label=\"left\")\n",
    "    right_id = drawGraph(graph, tree.right)    \n",
    "    graph.edge(node_id, right_id, label=\"right\")\n",
    "    return node_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Titanic Demo\n",
    "\n",
    "We are going to test our decision tree package with the dataset from the Titanic data science competition on Kaggle.  For more information, please visit the competition [home page](https://www.kaggle.com/c/titanic).  Here is the data.\n",
    "<pre>\n",
    "VARIABLE DESCRIPTIONS:\n",
    "survival        Survival (0 = No; 1 = Yes)\n",
    "pclass          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "name            Name\n",
    "sex             Sex\n",
    "age             Age\n",
    "sibsp           Number of Siblings/Spouses Aboard\n",
    "parch           Number of Parents/Children Aboard\n",
    "ticket          Ticket Number\n",
    "fare            Passenger Fare\n",
    "cabin           Cabin\n",
    "embarked        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "</pre>\n",
    "\n",
    "We first perform some feature engineering to fill missing data and create some new features.\n",
    "1. fill the missing age with the mdeian\n",
    "2. fill the missing embarked location with the mode\n",
    "3. create the `hasCabin` feature that indicates whether a passenger has cabin\n",
    "4. create the `familySize` feature which adds up `SibSp` and `Parch`\n",
    "\n",
    "As our training pool is small (around 900 samples), and the no. of features is small, we use the following model parameters to train a tree to prevent overfitting:\n",
    "1. maximum tree depth: 4\n",
    "2. minimum leaf size: 20\n",
    "\n",
    "Here is the resulting tree:\n",
    "<img src=\"./titanic_lv4.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareTitanic(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode().iloc[0])\n",
    "    df[\"hasCabin\"] = df[\"Cabin\"].apply(lambda x: \"true\" if pd.notnull(x) else \"false\")\n",
    "    df[\"familySize\"] = df[\"SibSp\"] + df[\"Parch\"]\n",
    "    \n",
    "    X = df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \n",
    "            \"hasCabin\", \"familySize\"]]\n",
    "    y = df[\"Survived\"]\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 13.44\n",
      "Accuracy 81.93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'64ec8d2392e14c1e84b806210fcc5648'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# load the traning data\n",
    "X, y = prepareTitanic(\"train.csv\")\n",
    "start_time = time.time()\n",
    "# train a tree\n",
    "tree = buildTree(X, y, maxDepth=4, minLeafSize=20)\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"Time elapsed: %.2f\" % elapsed_time)\n",
    "# score the accuracy\n",
    "accuracy = scoreTree(X, y, tree)\n",
    "print (\"Accuracy %.2f\"%accuracy)\n",
    "\n",
    "graph = Digraph(comment=\"Titanic Survival Descision Tree\")\n",
    "drawGraph(graph, tree)\n",
    "\n",
    "# uncomment to draw your tree\n",
    "#graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final Thoughts\n",
    "\n",
    "I tried to generate a submission for the Kaggle Titan competition on the testing data with this decision tree model, and it scored 78.947%. If we grow the tree into more depths, say maximum depth 5, the accuracy for training set data would be higher. But in contrast, the accuracy for unseen testing data (77.512%) would be lower. In machine learning language, it is called overfitting.  Although our decision tree model result was better than simply assuming only all female passengers survive (76.555%), the generalization ability for a single decision tree model is limited.  In the next iPython Notebook, we will see how to improve the prediction performance by building multiple trees, i.e. random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
